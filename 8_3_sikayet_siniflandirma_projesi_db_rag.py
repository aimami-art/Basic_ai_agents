"""
kullanicidan gelen ÅŸikayetlerin analiz edilmesi, kategorize edilmesi, 
ilgili birime yÃ¶nlendirilmesi ve uygun yanÄ±t verilmesi sistemi gerceklestirelim.

+ otomatik mail (simulasyon) + raporlama (.excel)
+ RAG with db
+ memory 

"""

from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document

from dotenv import load_dotenv
import os, warnings, sqlite3

warnings.filterwarnings("ignore")
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

def dbden_veri_al(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT baslik, icerik FROM urun_bilgileri")
    rows = cursor.fetchall()
    conn.close()
    return [Document(page_content=f"{baslik}:{icerik}") for baslik, icerik in rows]

# --- LLM & Embedding modeli
llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=api_key, temperature=0.2)
embedding_model = OpenAIEmbeddings(openai_api_key=api_key)

# --- Memory eklendi
memory = ConversationBufferMemory(return_messages=True)

# --- 1. ÃœrÃ¼n Bilgilerini yÃ¼kle & FAISS index oluÅŸtur
 
documents = dbden_veri_al("urun_bilgileri.db")
vectorstore = FAISS.from_documents(documents, embedding_model)
retriever = vectorstore.as_retriever()

# --- 2. Retrieval destekli QA zinciri (RAG)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    verbose=False
)

# --- 3. Åikayet ÅŸemasÄ±
schemas = [
    ResponseSchema(name="kategori", description="kargo, iade, Ã¶deme, teknik, iletiÅŸim"),
    ResponseSchema(name="yonlendirme", description="Ä°lgili departman"),
    ResponseSchema(name="cevap", description="KÄ±sa kullanÄ±cÄ± yanÄ±tÄ±"),
]
parser = StructuredOutputParser.from_response_schemas(schemas)
format_instructions = parser.get_format_instructions()

# --- 4. Prompt Template (memory geÃ§miÅŸi dahil)
prompt_template = PromptTemplate(
    input_variables=["sikayet", "bilgi", "gecmis"],
    partial_variables={"format_instructions": format_instructions},
    template="""
AÅŸaÄŸÄ±da geÃ§miÅŸ konuÅŸmalar yer almakta: geÃ§miÅŸi deÄŸerlendirerek de cevap ver, geÃ§miÅŸ sorulursa buradan cevap ver.

{gecmis}

KullanÄ±cÄ±dan yeni bir ÅŸikayet geldi. Åikayeti analiz et (kategori, yÃ¶nlendirme, kÄ±sa yanÄ±t).
AyrÄ±ca Ã¼rÃ¼n bilgisi aÅŸaÄŸÄ±dadÄ±r:

{bilgi}

{format_instructions}

Åikayet: {sikayet}
"""
)

# --- Sonsuz dÃ¶ngÃ¼: Ã§oklu ÅŸikayet
while True:
    sikayet = input("\nğŸ—£ Åikayetinizi yazÄ±nÄ±z (Ã‡Ä±kmak iÃ§in 'q'): ")
    if sikayet.lower() in ['q', 'exit', 'Ã§Ä±k']:
        break

    # --- RAG bilgisi al
    bilgi = rag_chain.run(sikayet)

    # --- GeÃ§miÅŸi al (formatla)
    # sohbet_gecmisi = memory.buffer BU ÅEKÄ°LDE YAZARSAN TÃœM GEÃ‡MÄ°ÅÄ° (HUMAN,AI,..) BÄ°RLEÅTÄ°RÄ°P STR ÅEKLÄ°NDE VERÄ°R VE m.type GÄ°BÄ° KOD YAZAMAZSIN
    sohbet_gecmisi = memory.chat_memory.messages
    gecmis = "\n".join([f"{m.type.capitalize()}: {m.content}" for m in sohbet_gecmisi])

    print("gecmis: ", gecmis)
    # --- Prompt oluÅŸtur
    prompt = prompt_template.format(sikayet=sikayet, bilgi=bilgi, gecmis=gecmis)
    response = llm.invoke(prompt)
    result = parser.parse(response.content)

    # --- BelleÄŸe kaydet
    memory.chat_memory.add_user_message(sikayet)
    memory.chat_memory.add_ai_message(response.content)

    # --- SonuÃ§larÄ± gÃ¶ster
    print("\n--- Åikayet Analizi ---")
    print("Kategori:", result["kategori"])
    print("YÃ¶nlendirme:", result["yonlendirme"])
    print("Cevap:", result["cevap"])

    # --- E-posta simÃ¼lasyonu
    print("\nğŸ“¬ Otomatik E-posta")
    print("Kime: kullanici@example.com")
    print("Konu: Åikayetiniz HakkÄ±nda")
    print(result["cevap"])